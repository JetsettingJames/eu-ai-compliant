# app/vector_processing.py

from typing import List, Dict, Any, Optional

# Langchain and ChromaDB specific imports
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
import chromadb # For client and persistence
import uuid # For generating document IDs if needed

from app.models import FuzzyMatchResult, RepositoryFile # Keep FuzzyMatchResult, RepositoryFile might be adapted
# LLMService might not be needed directly here if embeddings are handled by Langchain
# from app.services.llm_service import LLMService 
from app.config import settings
from app.logger import get_logger

logger = get_logger(__name__)

# --- Langchain and ChromaDB Setup ---

# Initialize OpenAI Embeddings model
# This will use OPENAI_API_KEY and EMBEDDING_MODEL from settings implicitly if they are set in the environment
# or if the OpenAI client is configured globally.
embeddings_model = OpenAIEmbeddings(model=settings.EMBEDDING_MODEL, openai_api_key=settings.OPENAI_API_KEY)

# Initialize ChromaDB client
# This sets up a persistent client. Data will be saved to the path specified.
# If the directory doesn't exist, Chroma will create it.
chroma_client = chromadb.PersistentClient(path=settings.CHROMA_PERSIST_PATH)

# Get or create the collection in ChromaDB
# The embedding_function is automatically managed by Langchain's Chroma wrapper when using from_documents or add_documents
# but when using chromadb client directly, it's good to be aware.
# For Langchain's Chroma vectorstore, it handles the embedding function internally.
vector_store = Chroma(
    client=chroma_client,
    collection_name=settings.CHROMA_COLLECTION_NAME,
    embedding_function=embeddings_model
)

logger.info(f"ChromaDB vector store initialized. Collection: '{settings.CHROMA_COLLECTION_NAME}', Path: '{settings.CHROMA_PERSIST_PATH}'")

# --- Text Chunking using Langchain --- 
# Adjusted default chunk size and overlap to be more token-aware for LLMs
# These can be further tuned based on the embedding model and content characteristics.
DEFAULT_LANGCHAIN_CHUNK_SIZE = 1000 # Characters, Langchain splitters often work with char count
DEFAULT_LANGCHAIN_CHUNK_OVERLAP = 100  # Characters

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=DEFAULT_LANGCHAIN_CHUNK_SIZE,
    chunk_overlap=DEFAULT_LANGCHAIN_CHUNK_OVERLAP,
    length_function=len,
    add_start_index=True, # Helpful for referencing original content
)

# --- Embedding and Upserting to ChromaDB --- 

async def upsert_repository_documents(
    repo_files: List[RepositoryFile],
    # llm_service: LLMService, # No longer needed directly if using Langchain's OpenAIEmbeddings
) -> List[str]: # Returns list of document IDs added to Chroma
    """Processes repository files, creates Langchain Documents, and upserts them to ChromaDB."""
    all_documents: List[Document] = []

    for repo_file in repo_files:
        if not repo_file.content or not isinstance(repo_file.content, str):
            logger.warning(f"Skipping empty or invalid content for source: {repo_file.path}")
            continue
        
        # Split the content using Langchain's text splitter
        chunks = text_splitter.split_text(repo_file.content)
        
        for i, chunk_text_content in enumerate(chunks):
            # Metadata for each document is crucial for filtering and retrieval
            doc_metadata = {
                "source_type": "repository_file",
                "source_identifier": repo_file.path, # e.g., file path
                "file_type": repo_file.file_type, # e.g., 'code', 'doc'
                "chunk_index": i,
                # 'start_index' is automatically added by RecursiveCharacterTextSplitter if add_start_index=True
            }
            # Create a Langchain Document
            # The content goes into page_content. Embeddings are generated by Chroma on add.
            doc = Document(
                page_content=chunk_text_content,
                metadata=doc_metadata
            )
            all_documents.append(doc)
    
    if not all_documents:
        logger.info("No documents generated from repository content to upsert.")
        return []

    logger.info(f"Attempting to upsert {len(all_documents)} documents from repository content to ChromaDB.")
    
    # Add documents to Chroma. Langchain's Chroma wrapper handles embedding generation.
    # It's usually a synchronous operation with the default Chroma client wrapper in Langchain,
    # but adding documents can be batched.
    # If ChromaDB client has async methods, we might explore them, but Langchain's default is sync.
    try:
        # Ensure vector_store.add_documents is awaited if it's an async operation
        # The mock in tests uses AsyncMock, implying the actual might be async too.
        added_doc_ids = await vector_store.add_documents(documents=all_documents)
        if added_doc_ids:
            logger.info(f"Successfully upserted {len(added_doc_ids)} repository documents to ChromaDB. IDs: {added_doc_ids}")
            return added_doc_ids
        else:
            # This case might occur if add_documents returns None or empty list on success with no new IDs
            logger.info("Upsert operation to ChromaDB completed, no new document IDs returned or all documents already existed.")
            return [] # Return empty list to signify no new IDs or confirm operation completed
    except Exception as e:
        logger.error(f"Error upserting repository documents to ChromaDB: {e}")
        # Optionally, re-raise or handle as per application's error strategy
        # For now, returning empty list to indicate failure or no additions
        return []

async def upsert_obligation_documents(
    obligations_data: Dict[str, Any], # Parsed from obligations.yaml
    # llm_service: LLMService # No longer needed
) -> List[str]: # Returns list of document IDs added to Chroma
    """Extracts text from obligations data, creates Langchain Documents, and upserts them to ChromaDB."""
    all_documents: List[Document] = [] # List of Langchain Document objects

    # obligations_data is expected to have a key "obligations" which is a LIST of dicts
    for obligation_item in obligations_data.get("obligations", []):
        if not isinstance(obligation_item, dict):
            logger.warning(f"Skipping obligation item, not a dictionary: {obligation_item}")
            continue

        # Construct document content from obligation fields
        # Ensure all expected keys are present or handle their absence gracefully
        title = obligation_item.get("title", "No Title")
        description = obligation_item.get("description", "No Description")
        details = obligation_item.get("details", "")
        reference_article = obligation_item.get("reference_article", "")
        responsible_entity = obligation_item.get("responsible_entity", "")

        # Combine relevant text fields for the document content
        text_content = f"Title: {title}\nDescription: {description}\nDetails: {details}\nReference: {reference_article}\nResponsible: {responsible_entity}"
        
        metadata = {
            "source_type": "obligation",
            "obligation_id": obligation_item.get("id", "UNKNOWN_ID"),
            "obligation_title": title,
            # Add other relevant metadata from obligation_item if needed
        }
        doc = Document(page_content=text_content, metadata=metadata)
        all_documents.append(doc)

    if not all_documents:
        logger.info("No obligation documents to upsert.")
        return []

    try:
        # Ensure vector_store.add_documents is awaited
        added_doc_ids = await vector_store.add_documents(documents=all_documents)
        if added_doc_ids:
            logger.info(f"Successfully upserted {len(added_doc_ids)} obligation documents to ChromaDB. IDs: {added_doc_ids}")
            return added_doc_ids
        else:
            logger.info("Upsert operation for obligations to ChromaDB completed, no new document IDs returned.")
            return []
    except Exception as e:
        logger.error(f"Error upserting obligation documents to ChromaDB: {e}")
        return []


async def find_matching_obligations_for_repo_doc(
    repo_doc_content: str,
    repo_doc_metadata: Dict[str, Any], # e.g., {'source_identifier': 'path/to/file.py', 'source_type': 'repository_file'}
    k: int = 5 # Default value since FUZZY_MATCH_K_VALUE is no longer in settings
) -> List[FuzzyMatchResult]:
    """Finds matching obligation documents from ChromaDB based on repository document content."""
    if not repo_doc_content:
        logger.warning("Received empty repository document content, skipping similarity search.")
        return []

    try:
        # Perform similarity search against obligations
        # Ensure vector_store.similarity_search_with_score is awaited
        results_with_scores = await vector_store.similarity_search_with_score(
            query=repo_doc_content,
            k=k,
            filter={"source_type": "obligation"} # Filter to search only within obligation documents
        )

        fuzzy_matches: List[FuzzyMatchResult] = []
        if not results_with_scores: # Check if results_with_scores is None or empty
            logger.info(f"No similarity search results for repo content from '{repo_doc_metadata.get('source_identifier', 'unknown source')}'.")
            return []

        for obligation_doc, score in results_with_scores:
            # score from Chroma is a distance metric (lower is better). 
            # Langchain's similarity_search_with_score for Chroma typically returns distance.
            # We need to define if 'score' here is similarity (higher is better) or distance.
            # Assuming it's distance for now as per Chroma's default. Let's convert to similarity if needed or adjust threshold logic.
            # For this example, let's assume 'score' is already a similarity score (0 to 1) for simplicity, 
            # or that the threshold is set appropriately for distance.
            # If it's distance, a common way to convert to similarity_score (0 to 1) is 1 / (1 + distance) or 1 - distance (if distance is normalized 0-1)
            # Let's assume the score returned by Chroma (or its Langchain wrapper) is directly usable or comparable to SIMILARITY_THRESHOLD
            # The test mock returns a score that is compared with SIMILARITY_THRESHOLD, implying higher is better.

            similarity_score = float(score) # Ensure score is float

            # Default similarity threshold of 0.75 since SIMILARITY_THRESHOLD is no longer in settings
            if similarity_score >= 0.75:
                match = FuzzyMatchResult(
                    obligation_id=obligation_doc.metadata.get("obligation_id", "UNKNOWN_ID"),
                    obligation_title=obligation_doc.metadata.get("obligation_title", "Unknown Title"),
                    repo_content_source=repo_doc_metadata.get("source_identifier", "unknown_source"),
                    repo_content_snippet=repo_doc_content[:500], # Store a snippet, default 500 chars since SNIPPET_MAX_LENGTH is no longer in settings
                    similarity_score=similarity_score
                )
                fuzzy_matches.append(match)
        
        if fuzzy_matches:
            logger.info(f"Found {len(fuzzy_matches)} matching obligations for repo content from '{repo_doc_metadata.get('source_identifier', 'unknown source')}'.")
        else:
            logger.info(f"No obligations met similarity threshold for repo content from '{repo_doc_metadata.get('source_identifier', 'unknown source')}'.")
        return fuzzy_matches

    except Exception as e:
        logger.error(f"Error during similarity search for repo content from '{repo_doc_metadata.get('source_identifier', 'unknown source')}': {e}")
        return []

# Old functions to be removed or fully replaced:
# - chunk_text (replaced by Langchain's text_splitter.split_text)
# - calculate_cosine_similarity (similarity handled by ChromaDB/Langchain)
# - embed_repository_content_chunks (replaced by upsert_repository_documents)
# - embed_obligations_data (replaced by upsert_obligation_documents)

logger.info("Vector processing module (Langchain & ChromaDB) loaded.")
